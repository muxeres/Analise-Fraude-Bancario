{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muxeres/Analise-Fraude-Bancario/blob/master/C%C3%B3pia_de_%5BONLINE%5D_Exemplo_2_Agente_de_di%C3%A1logo_baseado_em_regras_e_dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIMZgaR3IqM"
      },
      "source": [
        "# Exemplo 2 - Agente de diálogo híbrido (baseado em regras e dados)\n",
        "## Tecnólogo em Inteligência Artificial Aplicada - Agentes Conversacionais\n",
        "Neste notebook iremos construir um agente de diálogo que trará ocorrências sobre determinado tema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDw6X5Dq3h_Z"
      },
      "source": [
        "### Qual o contexto de nosso agente?\n",
        "\n",
        "Iremos desenvolver um agente de diálogo de question answering, que baseado em um corpus de texto sobre um assunto, trará as informações mais relevantes de acordo com a consulta do usuário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlqXcfuAPbQ"
      },
      "source": [
        "### Quais ferramentas e técnicas iremos utilizar?\n",
        "\n",
        "*   **NLTK** - O mais famoso toolkit de Processamento de Linguagem Natural em Python.\n",
        "*   **Expressões Regulares** - o pacote de regex do Python será utilizado para otimizar a busca de padrões.\n",
        "*   **urllib e BeautifulSoup** - Bibliotecas para obter dados de páginas HTML.\n",
        "*   **scikit-learn** - Pacote com funcionalidades de manipulaçã de dados e Machine Learning, vamos utilizar TF-IDF e Similaridade de cosseno.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHKrJBjdBqw1"
      },
      "source": [
        "### Construindo o agente de diálogo\n",
        "Nosso agente vai operar da seguinte maneira:\n",
        "\n",
        "1.   Recebe **entrada** do usuário\n",
        "2.   **Pré-processa** a entrada do usuário\n",
        "3.   Calcula a **similaridade** entre a entrada e as sentenças do corpus\n",
        "4.   Obtém a sentença **mais similar do corpus**\n",
        "5.   Mostra-a como **resposta** ao usuário\n",
        "\n",
        "Anteriormente a estas etapas, iremos criar nosso corpus ao obter dados da Wikipedia, automaticamente, então vamos lá!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb2Dp_OFJAyO"
      },
      "source": [
        "#### Importando as bibliotecas\n",
        "Vamos importar o pacote de expressões regulares do Python e também o acesso ao WordNet dado pelo NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRDAwuHdJZoo",
        "outputId": "8195f943-b92d-4254-9a34-e7bc25c1e960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')# Stemming em pt-br\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')# Lista de stopwords\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMVzxB8RB-F"
      },
      "source": [
        "#### Construindo o corpus\n",
        "Vamos fazer um *web-scraping* para obter os dados automaticamente da wikipedia. Este processo precisa ser executado apenas uma vez, e o arquivo salvo em forma de texto na máquina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UPAfE57RCIu"
      },
      "source": [
        "# Aqui buscamos a página sobre o BRASIL. Caso queira mudar o tema basta colocar o link para outra página.\n",
        "# Você pode também optar por obter dados de várias páginas diferentes, basta definir uma lista de links e iterar sobre elas.\n",
        "codigo_html = urllib.request.urlopen('https://pt.wikipedia.org/wiki/Brasil')\n",
        "codigo_html = codigo_html.read()\n",
        "\n",
        "# Processa o código HTML lido\n",
        "html_processado = bs.BeautifulSoup(codigo_html, 'lxml')\n",
        "\n",
        "# Busca todos parágrafos do texto\n",
        "paragrafos = html_processado.find_all('p')\n",
        "\n",
        "texto = ''\n",
        "\n",
        "# Percorre parágrafos e concatena textos\n",
        "for p in paragrafos:\n",
        "  texto += p.text\n",
        "\n",
        "# Normaliza para minúsculas\n",
        "texto = texto.lower()\n",
        "texto[0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjAXpxbwT5IB"
      },
      "source": [
        "##### Pré-processando o corpus\n",
        "Como você pode ver no trecho acima, precisamos remover caracteres especiais do texto, além de dividí-lo em sentenças válidas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS7MKMOCT5XM"
      },
      "source": [
        "texto = re.sub(r'\\[[0-9]*\\]', ' ', texto)\n",
        "texto = re.sub(r'\\s+', ' ', texto)\n",
        "texto[0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FGlLHzSUU7N"
      },
      "source": [
        "sentencas = nltk.sent_tokenize(texto, language='portuguese')\n",
        "palavras = nltk.word_tokenize(texto, language='portuguese')\n",
        "sentencas[10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65nR3LBTVmy0"
      },
      "source": [
        "#### Funções de pré-processamento de entrada do usuário\n",
        "Vamos criar funções para pré-processar as entradas do usuário, vamos retirar pontuações e usar Stemming nos textos, para que palavras similares sejam processadas de maneira igual pelo algoritmo (e.g., pedra e pedregulho teriam a mesma forma léxica)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjMnCjqfVm6N"
      },
      "source": [
        "# Define uma função que faz Stemming em todo um texto\n",
        "def stemming(tokens):\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  novotexto = []\n",
        "  for token in tokens:\n",
        "    novotexto.append(stemmer.stem(token.lower()))\n",
        "  return novotexto\n",
        "\n",
        "# Função que remove pontuação\n",
        "removePontuacao = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def preprocessa(documento):\n",
        "  return stemming(nltk.word_tokenize(documento.lower().translate(removePontuacao), language='portuguese'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UysIjiIPW6nT"
      },
      "source": [
        "# Veja como fica um texto após o pré-processamento\n",
        "preprocessa(\"Olá meu nome é Lucas, eu moro no Brasil, e você?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUL4YvxMYMVc"
      },
      "source": [
        "#### Resposta à saudações\n",
        "Mesmo que estejamos construindo um sistema de diálogo (baseado em tarefas), é normal que o usuário inicie conversas com saudações ao agente. Portanto, iremos desenvolver rapidamente uma função (i.e., regras) para lidar especialmente com esta situação.\n",
        "Vamos criar algumas respostas possíveis, e sempre vamos escolher aleatóriamente uma delas, para evitar que nosso agente fique repetitivo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJNLddZgYMeg"
      },
      "source": [
        "saudacoes_entrada = (\"olá\", \"bom dia\", \"boa tarde\", \"boa noite\", \"oi\", \"como vai\", \"e aí\")\n",
        "saudacoes_respostas = [\"olá\", \"olá, espero que esteja tudo bem contigo\", \"oi\", \"Oie\", \"Seja bem-vindo, em que posso te ajudar?\"]\n",
        "\n",
        "def geradorsaudacoes(saudacao):\n",
        "  for token in saudacao.split():\n",
        "    if token.lower() in saudacoes_entrada:\n",
        "      return random.choice(saudacoes_respostas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzW0X0iLZjBU"
      },
      "source": [
        "# Execute este exemplo várias vezes que verá diferentes respostas\n",
        "geradorsaudacoes('Olá')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_gdo4tjZ3X4"
      },
      "source": [
        "#### Resposta à consultas do usuário\n",
        "Agora, teremos uma função para lidar com consultas do usuário. Onde iremos comparar a similaridade entre a entrada do usuário, com as sentenças do corpus. Caso encontremos, a sentença mais similar será mostrada como resposta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zITJfxB0Z3ee"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DczIGmyUaPMr"
      },
      "source": [
        "def geradorrespostas(entradausuario):\n",
        "  resposta = ''\n",
        "  sentencas.append(entradausuario)\n",
        "\n",
        "  word_vectorizer = TfidfVectorizer(tokenizer=preprocessa, stop_words=stopwords.words('portuguese'))\n",
        "  all_word_vectors = word_vectorizer.fit_transform(sentencas)\n",
        "  similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "  similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "  matched_vector = similar_vector_values.flatten()\n",
        "  matched_vector.sort()\n",
        "  vector_matched = matched_vector[-2]\n",
        "\n",
        "  if vector_matched == 0:\n",
        "    resposta = resposta + \"Me desculpe, não entendi o que você pediu.\"\n",
        "    return resposta\n",
        "  else:\n",
        "    resposta = resposta + sentencas[similar_sentence_number]\n",
        "    return resposta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VQILIt-bgJ8"
      },
      "source": [
        "#### Interagindo com o agente de diálogo\n",
        "Vamos definir um algoritmo que continue interagindo com o usuário até que ele decida finalizar.\n",
        "\n",
        "O resultado não é sempre o ideal, mas já cobre muitas possíveis perguntas. Se utilizássemos apenas regras de diálogo para responder perguntas sobre um tema, precisaríamos de centenas de regras para tal. Mas como baseamos nossas respostas em dados apenas uma regra que calcula similaridade com nosso corpus já é o suficiente.\n",
        "\n",
        "Faça perguntas como:\n",
        "*  *Qual o esporte mais popular no Brasil?*\n",
        "*  *Quais eventos esportivos o Brasil já organizou?*\n",
        "*  *Como é a cozinha brasileira?*\n",
        "*  *Onde são realizadas pesquisas tecnológicas no Brasil?*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMIT3KygbgTe"
      },
      "source": [
        "continue_dialogue = True\n",
        "print(\"Olá, eu sou o Agente Tupiniquim. Me pergunte qualquer coisa sobre nosso país.\")\n",
        "while (continue_dialogue == True):\n",
        "  # Obtém entrada do usuário\n",
        "  human_text = input().lower()\n",
        "\n",
        "  if human_text != 'tchau':\n",
        "    if human_text == 'obrigado' or human_text == 'muito obrigado' or human_text == 'agradecido':\n",
        "      continue_dialogue = False\n",
        "      print(\"Agente Tupiniquim: Disponha\")\n",
        "    else:\n",
        "      if geradorsaudacoes(human_text) != None:\n",
        "        print(\"Agente Tupiniquim: \" + geradorsaudacoes(human_text))\n",
        "      else:\n",
        "        print(\"Agente Tupiniquim: \", end=\"\")\n",
        "        print(geradorrespostas(human_text))\n",
        "        sentencas.remove(human_text)\n",
        "  else:\n",
        "    continue_dialogue = False\n",
        "    print(\"Agente Tupiniquim: Até a próxima.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMOce1PXp1h"
      },
      "source": [
        "### O que pode ser feito?\n",
        "Utilizamos um modelo baseado em regras, onde uma das regras faz uso de um corpus de dados para formular as respostas, o que já deixou nosso modelo bem mais flexível, sem necessidade de criação de centenas/milhares de regras.\n",
        "\n",
        "Mas, o que poderíamos fazer para melhorar?\n",
        "\n",
        "\n",
        "*   Poderíamos obter não apenas os parágrafos(`<p>`) na página da wikipedia, mas também utilizar os dados dispostos na coluna direita, que apresentam informações bem relevantes como população, atual presidente, etc., para montar sentenças.\n",
        "*   Poderíamos melhorar ainda mais o cálculo de similaridade ao utilizar um modelo de Word Embeddings, além do TF-IDF.\n",
        "*   Obter dados sobre o Brasil de diferentes fontes.\n",
        "*   Criar um classificador de contexto para o agente, e de maneira dinâmica buscar páginas da Wikipedia correspondentes à pergunta do usuário, e só então dar a resposta. Assim nosso agente não ficaria limitado a perguntas sobre o Brasil.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP59ECc53nqF"
      },
      "source": [
        "## Referências e Material complementar\n",
        "\n",
        "* [Python for NLP: Creating a Rule-Based Chatbot](https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/)\n",
        "* [Building a Simple Chatbot from Scratch in Python (using NLTK)](https://morioh.com/p/6cc33336784c)\n",
        "* [Building a simple chatbot in python](https://medium.com/nxtplus/building-a-simple-chatbot-in-python-3963618c490a)\n",
        "* [Designing A ChatBot Using Python: A Modified Approach](https://towardsdatascience.com/designing-a-chatbot-using-python-a-modified-approach-96f09fd89c6d)\n",
        "* [Build Your First Python Chatbot Project](https://dzone.com/articles/python-chatbot-project-build-your-first-python-pro)\n",
        "* [Python Chatbot Project – Learn to build your first chatbot using NLTK & Keras](https://data-flair.training/blogs/python-chatbot-project/)\n",
        "* [Python Chat Bot Tutorial - Chatbot with Deep Learning (Part 1)](https://www.youtube.com/watch?v=wypVcNIH6D4)\n",
        "* [Intelligent AI Chatbot in Python](https://www.youtube.com/watch?v=1lwddP0KUEg)\n",
        "* [Coding a Jarvis AI Using Python 3 For Beginners](https://www.youtube.com/watch?v=NZMTWBpLUa4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwExBlA63rt9"
      },
      "source": [
        "Este notebook foi produzido por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500)."
      ]
    }
  ]
}